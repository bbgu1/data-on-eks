apiVersion: ray.io/v1
kind: RayService
metadata:
  name: vllm
  namespace: rayserve-vllm
spec:
  serveConfigV2: |
    applications:
      - name: llama3
        import_path: "vllm_openai_serve:deployment"
        route_prefix: "/llama3"
        runtime_env:
          env_vars:
            MODEL_ID: "meta-llama/Meta-Llama-3-8B-Instruct"
            GPU_MEMORY_UTILIZATION: "0.9"
            MAX_MODEL_LEN: "4096"
            MAX_NUM_SEQ: "4"
            MAX_NUM_BATCHED_TOKENS: "32768"
            NUM_OF_GPU: "2"
            VLLM_ATTENTION_BACKEND: "XFORMERS"
        deployments:
          - name: VLLMDeployment
            autoscaling_config:
              metrics_interval_s: 0.2
              min_replicas: 1
              max_replicas: 4
              look_back_period_s: 2
              downscale_delay_s: 600
              upscale_delay_s: 30
              target_num_ongoing_requests_per_replica: 20
            graceful_shutdown_timeout_s: 5
            max_concurrent_queries: 100
            ray_actor_options:
              num_cpus: 4
              num_gpus: 2
  
      