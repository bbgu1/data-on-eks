apiVersion: ray.io/v1
kind: RayService
metadata:
  name: vllm
  namespace: rayserve-vllm
spec:
  serveConfigV2: |
    applications:
      - name: mistral
        import_path: "vllm_serve:deployment"
        route_prefix: "/mistral"
        runtime_env:
          env_vars:
            MODEL_ID: "mistralai/Mistral-7B-Instruct-v0.2"
            GPU_MEMORY_UTILIZATION: "0.9"
            MAX_MODEL_LEN: "8192"
            MAX_NUM_SEQ: "4"
            MAX_NUM_BATCHED_TOKENS: "32768"
            VLLM_ATTENTION_BACKEND: "XFORMERS"
        deployments:
          - name: VLLMDeployment
            autoscaling_config:
              metrics_interval_s: 0.2
              min_replicas: 1
              max_replicas: 4
              look_back_period_s: 2
              downscale_delay_s: 600
              upscale_delay_s: 30
              target_num_ongoing_requests_per_replica: 20
              max_replica_per_node: 1
            graceful_shutdown_timeout_s: 5
            max_concurrent_queries: 100
            ray_actor_options:
              num_cpus: 4
              num_gpus: 1
  
      